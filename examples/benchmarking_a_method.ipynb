{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking a method\n",
    "\n",
    "When developing a new method, you might want to see how other methods perform\n",
    "on the dataset you are evaluating. Photoholmes includes the implementation for several\n",
    "methods from the literature, covering a diverse array of approaches to forgery detection\n",
    "to compare to.\n",
    "\n",
    "It also includes a Benchmark object to easily evaluate the performance of a method over\n",
    "a dataset. This ensures a fair and reproducible comparison between methods.\n",
    "This is a short tutorial on how to use Benchmark object the included method\n",
    "DQ and a custom method we will define on the Columbia dataset.\n",
    "\n",
    "1. [ Running an included method ](#running-an-included-method)\n",
    "2. [ Running a custom method ](#running-a-custom-method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an included method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run an included method, we first import and instantiate the method. You can do this by using the method factory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photoholmes.methods.method_factory import MethodFactory, MethodName\n",
    "\n",
    "\n",
    "dq, dq_preprocessing = MethodFactory.load(MethodName.DQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or by importing the method directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photoholmes.methods.dq import DQ, dq_preprocessing\n",
    "\n",
    "dq = DQ()\n",
    "dq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to import the dq_preprocessing pipeline, as we need to pass it to the Dataset. But first, we need to download the [Columbia Uncompressed Image Splicing Detection](https://www.ee.columbia.edu/ln/dvmm/downloads/authsplcuncmp/) dataset. Keep in mind this dataset is under a [ research-only use License ](https://www.ee.columbia.edu/ln/dvmm/downloads/authsplcuncmp/dlform.html). You can download the dataset [ here ](https://www.dropbox.com/sh/786qv3yhvc7s9ki/AACbEEzGPrD3_y38bpWHzgdqa?e=1&dl=0).\n",
    "\n",
    "Once downloaded, unzip the files and update the following variable with the path to the dataset folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columbia_dataset_path: str = (\n",
    "    \"data/Columbia Uncompressed Image Splicing Detection\"  # UPDATE WITH THE PATH ON YOUR COMPUTER\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as with the methods, we can load a dataset by direct import:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photoholmes.datasets.columbia import ColumbiaDataset\n",
    "\n",
    "dataset = ColumbiaDataset(\n",
    "    img_dir=columbia_dataset_path,\n",
    "    item_data=[\"image\", \"dct_coefficients\", \"qtables\"],\n",
    "    transform=dq_preprocessing,\n",
    ")\n",
    "print(\"Total images: \", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or using the factory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photoholmes.datasets.dataset_factory import DatasetFactory, DatasetName\n",
    "\n",
    "\n",
    "dataset = DatasetFactory.load(\n",
    "    DatasetName.COLUMBIA,\n",
    "    dataset_dir=columbia_dataset_path,\n",
    "    item_data=[\"image\", \"dct_coefficients\", \"qtables\"],\n",
    "    transform=dq_preprocessing,\n",
    ")\n",
    "print(\"Total images: \", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% FIXME add links and shit a nuestra propia documentación cuando este\n",
    "\n",
    "For more information on the datasets, see the Datasets section of the README.md\n",
    "\n",
    "Lastly, we need to select the metrics to evaluate. We will load the Auroc, IoU and F1 using the MetricFactory. To see how to use metrics outside the factory or custom metrics, see the documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photoholmes.metrics.metric_factory import MetricFactory, MetricName\n",
    "\n",
    "metrics = MetricFactory.load([MetricName.AUROC, MetricName.F1, MetricName.IoU])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to run the Benchmark. First, we create a Benchmark Object. The constructor allows to tune the following parameters:\n",
    "\n",
    "- **save_method_outputs:** Whether to save the method outputs.\n",
    "- **save_extra_outputs:** Whether to save extra outputs.\n",
    "- **save_metrics_flag:** Whether to save metrics.\n",
    "- **output_path:** Path to the output folder.\n",
    "- **device:** torch Device for computation.\n",
    "- **use_existing_output:** Whether to use existing saved outputs.\n",
    "- **verbose:** Verbosity level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photoholmes.benchmark import Benchmark\n",
    "\n",
    "benchmark = Benchmark(\n",
    "    output_folder=\"example_output\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to go! The following cell will run the evaluation. It should take around two minutes to continue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_results = benchmark.run(\n",
    "    method=dq,\n",
    "    dataset=dataset,\n",
    "    metrics=metrics,\n",
    ")\n",
    "print(dq_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a folder example_output has been created. There, the benchmark will create the following folder structure:\n",
    "\n",
    "```terminal\n",
    "example_output/\n",
    "└── dq\n",
    "    └── columbiadataset\n",
    "        ├── metrics\n",
    "        │   └── 20240308_01:34_tampered_and_pristine\n",
    "        │       ├── heatmap_report.json\n",
    "        │       └── heatmap_state.pt\n",
    "        └── outputs\n",
    "            ├── canong3_02_sub_01\n",
    "            │   └── output.npz\n",
    "            ├── canong3_02_sub_02\n",
    "            ...\n",
    "```\n",
    "\n",
    "Inside the _<method>/<dataset>_ folder, in this case _dq/colmubiadataset_, you will find two folders: _output/_ y _metrics_. Inside the ouputs\n",
    "folder you will find the saved model outputs, so they can be reused and save compute time. There are three types of output:\n",
    "\n",
    "1. _output.npz_: saves the benchmark outputs (heatmap, mask and/or detection)\n",
    "2. _output_extra.npz_: saves any extra output arrays that were included in the benchmark output. This will be included only if the benchmark has the\n",
    "   _save_extra_output=True_.\n",
    "3. _output_extra_json.json_: save any extra output that isn't an array.\n",
    "\n",
    "On the metrics folder, you will find the benchmarking results. Every time you run a benchmark, a folder with the name timestamp and the\n",
    "type of run (tampered only or tampered and pristine). Inside the folder, you will find metric files for each type of output your method outputs.\n",
    "Photoholmes divides method outputs into three types:\n",
    "\n",
    "1. **heatmap:** a probability map.\n",
    "2. **mask:** a binary mask.\n",
    "3. **detection:** a score for detection.\n",
    "\n",
    "Your method can predict only one output per type. Inside the metrics folder you will find a report for each type your method outputed, in this case\n",
    "only _heatmap_report.json_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You have benchmarked DQ on the columbia dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a custom method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's implemented a basic method to use our benchmark on. We won't do anything fancy, simply predict a random array in the same shape\n",
    "as the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Any, Tuple\n",
    "from photoholmes.methods.base import BaseMethod, BenchmarkOutput\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import torch\n",
    "from photoholmes.preprocessing.image import ToNumpy\n",
    "from photoholmes.preprocessing.input import InputSelection\n",
    "\n",
    "from photoholmes.preprocessing.pipeline import PreProcessingPipeline\n",
    "\n",
    "\n",
    "class RandomMethod(BaseMethod):\n",
    "\n",
    "    def __init__(self, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def predict(self, image: NDArray) -> Tuple[NDArray, float]:\n",
    "        heatmap = np.random.random(size=image.shape[:2])\n",
    "        detection = random.random()\n",
    "        return heatmap, detection\n",
    "\n",
    "    def benchmark(self, image: NDArray) -> BenchmarkOutput:\n",
    "        heatmap, detection = self.predict(image)\n",
    "        return {\n",
    "            \"heatmap\": torch.from_numpy(heatmap),\n",
    "            \"mask\": None,\n",
    "            \"detection\": torch.tensor([detection]),\n",
    "        }\n",
    "\n",
    "\n",
    "method = RandomMethod()\n",
    "random_method_preprocessing = PreProcessingPipeline(\n",
    "    [ToNumpy(), InputSelection([\"image\"])]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl https://media.taringa.net/knn/fit:550/Z3M6Ly9rbjMvdGFyaW5nYS9ELzIvNy9GLzQvRi9veWVjb21vdmFhLzQzNy5qcGc -o data/paul.webp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photoholmes.utils.image import read_image, plot\n",
    "\n",
    "img = read_image(\"data/paul.webp\")\n",
    "plot(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_preprocessed = random_method_preprocessing(image=img)\n",
    "heatmap, score = method.predict(**input_preprocessed)\n",
    "print(\"Detection score:\", score)\n",
    "plt.imshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our method clearly isn't good, but it serves as a good enough example.\n",
    "\n",
    "Let's load our datasets and metrics again and run the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photoholmes.datasets.dataset_factory import DatasetFactory, DatasetName\n",
    "from photoholmes.metrics.metric_factory import MetricFactory, MetricName\n",
    "\n",
    "\n",
    "columbia_dataset_path: str = (\n",
    "    \"data/Columbia Uncompressed Image Splicing Detection\"  # UPDATE WITH THE PATH ON YOUR COMPUTER\n",
    ")\n",
    "\n",
    "dataset = DatasetFactory.load(\n",
    "    DatasetName.COLUMBIA,\n",
    "    dataset_dir=columbia_dataset_path,\n",
    "    item_data=[\"image\", \"dct_coefficients\", \"qtables\"],\n",
    "    transform=random_method_preprocessing,\n",
    ")\n",
    "print(\"Total images: \", len(dataset))\n",
    "\n",
    "metrics = MetricFactory.load([MetricName.AUROC, MetricName.F1, MetricName.IoU])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photoholmes.benchmark import Benchmark\n",
    "\n",
    "benchmark = Benchmark(output_folder=\"example_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_method_results = benchmark.run(method=method, dataset=dataset, metrics=metrics)\n",
    "print(random_method_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check the output folder, you will notice a felder _randommethod/columbiadataset_ with the evaluation results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
